'''Validate all proxies in data dir.

proxy must be validated where it is used.
'''

import os
from os.path import dirname, abspath
import requests
import sys
import multiprocessing
from multiprocessing import Pool
import json
import time
import random
import gzip
import datetime


root_dir = dirname(dirname(abspath(__file__)))
data_dir = '{}/data'.format(root_dir)


def validate_proxies(out_dir, nproc=8, nretry=5,
                     timeout_connect=5, timeout_read=2, timesleep=1):
    '''Validate all proxies in data dir (generated by proxydocker.py)
    '''

    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M")
    out_name = '{}/{}.validated.list.gz'.format(out_dir, timestamp)

    # load all proxy list and black list
    proxy_list = set()
    black_list = set()
    njson = 0
    nblack = 0
    for fname in os.listdir(data_dir):
        fname = data_dir + '/' + fname
        if fname.endswith('.proxies.json.gz'):
            njson += 1
            data = json.loads(gzip.open(fname).read().decode('utf-8'))
            for proxy in data:
                proxy_list.add(proxy)
        elif fname.endswith('.proxies.blacklist'):
            nblack += 1
            for line in open(fname):
                black_list.add(line.rstrip())

    print('loaded {} json files'.format(njson), flush=True,
          file=sys.stderr)
    print('loaded {} black lists'.format(nblack), flush=True,
          file=sys.stderr)
    print('loaded {} raw proxies'.format(len(proxy_list)), flush=True,
          file=sys.stderr)
    print('loaded {} bad proxies'.format(len(black_list)), flush=True,
          file=sys.stderr)
    proxy_list = proxy_list - black_list
    print('test {} proxies'.format(len(proxy_list)), flush=True,
          file=sys.stderr)

    pool = multiprocessing.Pool(processes=nproc)
    res = []
    for proxy in sorted(proxy_list):
        if proxy in black_list:
            continue
        res.append(pool.apply_async(
            validate_one_proxy,
            args=(proxy,nretry, timeout_connect, timeout_read, timesleep)))

    output = [x.get() for x in res]
    pool.close()
    pool.join()

    print('writing ' + out_name, flush=True, file=sys.stderr)
    fout = gzip.open(out_name, 'wb')
    out_data = []
    for x in output:
        out_data.append('\t'.join([str(a) for a in x]))
    out_data = '\n'.join(out_data) + '\n'
    fout.write(out_data.encode('utf-8'))
    fout.close()


def validate_one_proxy(proxy, nretry, timeout_connect, timeout_read,
                       timesleep):
    ''' Test a proxy with a number of constraints
    '''

    test_url = 'http://icanhazip.com'
    timeout = (timeout_connect, timeout_read)

    print(':: testing {}'.format(proxy), flush=True, file=sys.stderr)
    npass = 0
    tlapse = 0
    for i in range(nretry):
        try:
            r = requests.get(test_url, proxies={'http':proxy},
                             timeout=timeout)
            if r.ok and proxy.split(':')[0] == r.text.rstrip():
                npass += 1
                tlapse += r.elapsed.total_seconds()
        except:
            pass
            # raise
        time.sleep(timesleep)

    return [proxy, npass, nretry-npass, tlapse/npass if npass else 0]










# def test_all_proxy(timestamp, domain='proxydocker', nproc=8):

#     json_in = '{}/{}.{}.proxies.json.gz'.format(data_dir, domain, timestamp)
#     list_out = '{}/{}.{}.proxies.valid.list'.format(data_dir, domain, timestamp)
#     data = json.loads(gzip.open(json_in).read().decode('utf-8'))

#     proxy_list = sorted(list(data.keys()))
#     print('validating {} proxies ...'.format(len(proxy_list)))
#     # proxy_list = [x for x in range(100)]
#     p = Pool(processes=nproc)
#     r = [p.apply_async(test_proxy, args=(x,)) for x in proxy_list]
#     output = [x.get() for x in r]
#     output = [x for x in output if x != 'Null']

#     print('writing ' + list_out, file=sys.stderr)
#     fout = open(list_out, 'w')
#     for proxy in output:
#         print(proxy[0], proxy[1], sep='\t', file=fout)
#     fout.close()


# def test_proxy(proxy_url, test_url='http://icanhazip.com',
#                timeout=15, nretry_max=3):
#     '''Test if proxy is effective and return elapse of test time.'''

#     npass = 0
#     for nretry in range(nretry_max):
#         print(':: testing {} ... (retry={}) [{}]'
#               .format(proxy_url, nretry, multiprocessing.current_process()),
#               file=sys.stderr, flush=True)
#         try:
#             r = requests.get(test_url, proxies={'http': proxy_url},
#                              timeout=timeout)
#             if r.text.rstrip() == proxy_url.split(':')[0]:




#     nretry = 0
#     while nretry < nretry_max:
#         print(':: testing {} ... (retry={}) [{}]'
#               .format(proxy_url, nretry, multiprocessing.current_process()),
#               file=sys.stderr, flush=True)
#         try:
#             r = requests.get(test_url, proxies={'http': proxy_url},
#                              timeout=timeout)
#             if r.ok and r.text.rstrip() == proxy_url.split(':')[0]:
#                 return proxy_url, 'PASS'
#         except:
#             pass

#         nretry += 1
#         time.sleep(1)

#     return proxy_url, 'FAIL'


if __name__ == '__main__':

    out_dir = sys.argv[1]
    validate_proxies(out_dir)
